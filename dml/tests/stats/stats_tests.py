import os
os.environ['NUMBA_DISABLE_JIT'] = '1'

from dml.stats import *
import numpy as np
from dml.utils.estimation import empirical_kl_divergence
from dml.stats import SequenceEncodableProbabilityDistribution, DataSequenceEncoder
import abc
import unittest
import pytest
import dml.utils.vector as vec

def str_eval_test(dist):    
    dist2 = eval(str(dist))
    return dist == dist2

def estimator_test(dist, est):
    # pseudo_count should be passed on estimator call
    return dist.estimator() == est

def dist_to_encoder_test(dist, encoder):
    return dist.dist_to_encoder() == encoder


def sampler_repeat_test(dist: SequenceEncodableProbabilityDistribution):
    """
    Test the repeatability of the sampler for a given distribution.

    This function tests whether the samples generated by the distribution's sampler
    are consistent when the same seed is used multiple times.

    Args:
        dist (SequenceEncodableProbabilityDistribution): The distribution instance to test, 
            which must have a `sampler` method.

    Returns:
        tuple: A tuple where the first element is a boolean indicating if the sampler 
               is repeatable, and the second element is a list of results for each seed.
    """
    seeds = [1, 2, 3]
    sz = 20
    rv = []
    for seed in seeds:
        s = dist.sampler(seed)
        d1 = s.sample(size=sz)
        s = dist.sampler(seed)
        d2 = s.sample(size=sz)

        is_same = [u[0] == u[1] for u in zip(map(str, d1), map(str, d2))]
        rv.append(all(is_same))

    return all(rv), rv


def log_density_test(dist: SequenceEncodableProbabilityDistribution, encoder: DataSequenceEncoder):
    """
    Test the log density function for a given distribution.

    This function tests that the log density values for a set of samples generated 
    by the distribution's sampler match the values computed using `seq_log_density`.

    Args:
        dist (SequenceEncodableProbabilityDistribution): The distribution instance to test.

    Returns:
        tuple: A tuple where the first element is a boolean indicating whether 
               the log density values are consistent, and the second element 
               is the maximum discrepancy between the calculated values.
    """
    seeds = [1, 2, 3]
    sz = 20
    rv = []
    for seed in seeds:
        s = dist.sampler(seed)
        data = s.sample(size=sz)
        try:
            enc_data = encoder.seq_encode(data)
        except:
            return False, "encoder.seq_encode(data)"
        seq_ll = dist.seq_log_density(enc_data)
        for i in range(sz):
            if seq_ll[i] == 0:
                seq_ll[i] = np.abs(dist.log_density(data[i]))
            else:
                seq_ll[i] = np.abs(seq_ll[i] - dist.log_density(data[i])) / np.abs(seq_ll[i])

        rv.append(max(seq_ll))
        
    return max(rv) < 1.0e-14, "max(rv) test"

def estimator_factory_test(x):
    est, factory = x
    rv = est.accumulator_factory()
    return rv == factory

def factory_make_test(x):
    factory, acc_ = x
    acc = factory.make()
    return acc == acc_

def acc_to_encoder_test(x):
    acc, encoder = x 
    return acc.acc_to_encoder() == encoder

def estimation_test(dist: SequenceEncodableProbabilityDistribution, encoder: DataSequenceEncoder):
    """
    Test the estimation of a distribution using empirical KL divergence.

    This function tests whether the empirical KL divergence between the distribution 
    and its estimator decreases as the sample size increases.

    Args:
        dist (SequenceEncodableProbabilityDistribution): The distribution instance to test, which 
            must have a `sampler` method and an `estimator` method.

    Returns:
        tuple: A tuple where the first element is a boolean indicating whether the 
               estimation improved as sample size increased, and the second element 
               is a list of KL divergence values for each sample size.
    """
    seeds = [1, 2, 3, 4]
    szs = [50, 500, 5000]
    rv = []

    akld = []
    for seed in seeds:
        kld = []
        better = []
        for sz in szs:
            data = dist.sampler(seed).sample(size=sz)
            est = dist.estimator()
            enc_data = seq_encode(data, encoder=encoder)
            est_dist = estimate(data, est, dist)

            emp_kld, _, _ = empirical_kl_divergence(dist, est_dist, enc_data)

            if len(kld) > 0:
                better.append(kld[-1] >= emp_kld)

            kld.append(emp_kld)
        akld.append(kld)
        rv.append(all(better))

    akld_mean = np.mean(akld, axis=0)
    rv = np.all(akld_mean[1:] <= akld_mean[:-1])

    return rv, akld


def seq_estimation_test(dist: SequenceEncodableProbabilityDistribution, encoder: DataSequenceEncoder):
    """
    Test the sequence estimation of a distribution using empirical KL divergence.

    This function tests whether the empirical KL divergence between the distribution
    and its sequence estimator decreases as the sample size increases.

    Args:
        dist (SequenceEncodableProbabilityDistribution): The distribution instance to test, 
            which must have a `sampler` method and an estimator capable of sequence estimation.

    Returns:
        tuple: A tuple where the first element is a boolean indicating whether the 
            sequence estimation improved as sample size increased, and the second 
            element is a list of KL divergence values for each sample size.
    """
    seeds = [1, 2, 3, 4]
    szs = [50, 500, 5000]
    rv = []

    akld = []
    for seed in seeds:
        kld = []
        better = []
        for sz in szs:
            data = dist.sampler(seed).sample(size=sz)
            est = dist.estimator()
            enc_data = seq_encode(data, encoder=encoder)
            est_dist = seq_estimate(enc_data, est, dist)

            emp_kld, _, _ = empirical_kl_divergence(dist, est_dist, enc_data)

            if len(kld) > 0:
                better.append(kld[-1] >= emp_kld)

            kld.append(emp_kld)
        akld.append(kld)
        rv.append(all(better))

    akld_mean = np.mean(akld, axis=0)
    rv = np.all(akld_mean[1:] <= akld_mean[:-1])

    return rv, akld

def initialize_test(dist, encoder):
    seeds = [1, 2, 3]
    sz = 1000
    rv = []
    for seed in seeds:
        data = dist.sampler(seed=seed).sample(sz)
        enc_data = encoder.seq_encode(data)

        est = dist.estimator()

        idata = iter(data)
        acc0 = est.accumulator_factory().make()
        rng = np.random.RandomState(seed)

        for _, x in enumerate(idata):
            acc0.initialize(x, 1.0, rng)
        v0 = acc0.value()

        acc1 = est.accumulator_factory().make()
        rng = np.random.RandomState(seed)

        acc1.seq_initialize(enc_data, np.ones(len(data)), rng)
        v1 = acc1.value()
        d0 = est.estimate(None, v0)
        d1 = est.estimate(None, v1) 

        ekld = empirical_kl_divergence(d0, d1, [(sz, enc_data)])
        rv.append(np.abs(ekld[0]) < 1.0e-1)
        
    return np.all(rv), rv


def seq_update_test(dist, encoder):
    seeds = [1, 2, 3]
    sz = 1000
    rv = []
    for seed in seeds:
        data = dist.sampler(seed=seed).sample(sz)
        enc_data = [(sz, encoder.seq_encode(data))]

        est = dist.estimator()
        rng = np.random.RandomState(seed)
        prev_estimate = seq_initialize(enc_data, est, rng)
        estimate = seq_estimate(enc_data, est, prev_estimate)

        ll_prev = np.sum(prev_estimate.seq_log_density(enc_data[0][1]))
        ll = np.sum(estimate.seq_log_density(enc_data[0][1]))
        log_diff = ll-ll_prev
        rv.append(log_diff >= 0)

    return np.all(rv), rv


class StatsTestClass(unittest.TestCase, metaclass=abc.ABCMeta):
    """
    Abstract class for Binomial test cases.
    Provides common test methods for subclasses to inherit.
    Subclasses must implement the `setUp` method to provide specific setup logic.
    """

    @abc.abstractmethod
    def setUp(self):
        """
        Abstract method for setting up test data.
        Subclasses must implement this method.
        """
        # self.eval_dists
        # self.dist_est
        # self.dist_encoder
        # self.sampler_dist
        # self.density_dist_encoder
        # self.est_factory
        # self.factory_acc
        # self.acc_encoder

        pass

    def test_01_str_eval(self):
        for dist in self.eval_dists:
            self.assertTrue(str_eval_test(dist))

    @pytest.mark.dependency(name="estimator")
    def test_02_estimator(self):
        for x in self.dist_est:
            rv = estimator_test(*x)
            self.assertTrue(rv)

    @pytest.mark.dependency(name="dist_to_encoder")
    def test_03_dist_to_encoder(self):
        for dist, encoder in self.dist_encoder:
            self.assertTrue(dist_to_encoder_test(dist, encoder))

    @pytest.mark.dependency(name="sampler")
    def test_04_sampler(self):
        res = sampler_repeat_test(self.sampler_dist)
        self.assertTrue(res[0], str(res[1]))

    @pytest.mark.dependency(depends=["sampler"], name="log_density")
    def test_05_log_density(self):
        for x in self.density_dist_encoder:
            res = log_density_test(*x)
            self.assertTrue(res[0], str(res[1]))

    @pytest.mark.dependency(name="estimator_factory")
    def test_06_estimator_factory(self):
        for est_factory in self.est_factory:
            rv = estimator_factory_test(est_factory)
            self.assertTrue(rv)

    @pytest.mark.dependency(name="factory_make")
    def test_07_factory_make(self):
        for x in self.factory_acc:
            rv = factory_make_test(x)
            self.assertTrue(rv)

    @pytest.mark.dependency(name="acc_to_encoder")
    def test_08_acc_to_encoder(self):
        for x in self.acc_encoder:
            rv = acc_to_encoder_test(x)
            self.assertTrue(rv)

    @pytest.mark.dependency(depends=["estimator", "log_density", "estimator_factory", "factory_make"])
    def test_09_seq_update(self):
        for x in self.density_dist_encoder:
            res = seq_update_test(*x)
            self.assertTrue(res[0], str(res[1]))

    # @pytest.mark.dependency(depends=["estimator", "log_density", "estimator_factory", "factory_make"])
    # def test_09_estimation(self):
    #     for x in self.density_dist_encoder:
    #         res = estimation_test(*x)
    #         self.assertTrue(res[0], str(res[1]))

    # @pytest.mark.dependency(depends=["estimator", "log_density", "estimator_factory", "factory_make"])
    # def test_11_initialize(self):
    #     for x in self.density_dist_encoder:
    #         res = initialize_test(*x)
    #         self.assertTrue(res[0], str(res[1]))

    
            
